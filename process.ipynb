{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "# EN\n",
    "# len: 4459532\n",
    "# key per line: dict_keys(['meta', 'text', 'input', 'output', 'instruction'])\n",
    "# 'meta' key: dict_keys(['Dataset', 'Gen', 'IFT', 'Lang', 'CFT-MR', 'CFT-P', 'CFT-SR', 'Task', 'original_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'ShareGPT', 'belle_cn', 'instinwild', 'Chinese-medical', 'alpacaGPT4', 'COIG', 'HC3', 'MOSS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "data_stat_path = \"/home/chenxiaojia/data/competition/competition_kit/data-juicer/outputs/v6_text/en_processed_stats.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/training_dataset.jsonl\"\n",
    "output = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/flitered_training_dataset.jsonl\"\n",
    "\n",
    "cnt = 0\n",
    "new_data_writer = jsonlines.open(output, 'w')\n",
    "\n",
    "with jsonlines.open(data_stat_path) as file1, jsonlines.open(data_path) as file2:\n",
    "    for data1, data2 in zip(file1, file2):\n",
    "        # process data from both files here\n",
    "        data1 = data1[\"__dj__stats__\"]\n",
    "        if data1[\"flagged_words_ratio\"] <= 0.017:\n",
    "            # print(data1[\"flagged_words_ratio\"])\n",
    "            # print(data2)\n",
    "            # print(\"\")\n",
    "            cnt += 1\n",
    "            new_data_writer.write(data2)\n",
    "\n",
    "new_data_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/raw_data/raw_data_en.jsonl\"\n",
    "\n",
    "dataset_type = {}\n",
    "task_type = {}\n",
    "reader = jsonlines.open(data_path, 'r')\n",
    "\n",
    "new_r = jsonlines.open(\"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/debug_dj/debug_1k\", 'w')\n",
    "\n",
    "cnt = 0\n",
    "for line in reader:\n",
    "    if cnt <= 1000:\n",
    "        new_r.write(line)\n",
    "        cnt += 1\n",
    "new_r.close()\n",
    "# for line in reader:\n",
    "#     ds_type = line['meta']['Dataset']\n",
    "#     tsk_type = line['meta']['Task']\n",
    "#     if dataset_type.get(ds_type, -1) == -1:\n",
    "#         dataset_type[ds_type] = 0\n",
    "#         task_type[tsk_type] = 0\n",
    "#     dataset_type[ds_type] += 1\n",
    "#     task_type[tsk_type] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in dataset_type.items():\n",
    "#     print(k, len(v))\n",
    "\n",
    "print(task_type)\n",
    "print(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('temp_data/sharegpt.json', 'w') as f:\n",
    "    json.dump(dataset_type['ShareGPT'], f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(open('temp_data/sharegpt.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM for evaluation data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ernie\n",
    "import erniebot\n",
    "\n",
    "erniebot.api_type = 'aistudio'\n",
    "erniebot.access_token = 'a335b39380dbc8aaaec76f78725d1dc867cf2d27'\n",
    "\n",
    "models = erniebot.Model.list()\n",
    "print(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following.\\nInstruction: [where did all the money that was lost in the sub prime mortgage crisis and subsequent global recession actually go ? Money does n't simply disappear so if people lost money then it must have gone somewhere . Where ? Explain like I'm five.]\\nInput: []\\nResponse: [The subprime mortgage crisis was a financial crisis that occurred in the late 2000s as a result of the collapse of the housing market. Many people had taken out mortgages to buy homes, but when the housing market crashed, the value of their homes decreased and they were no longer able to make their mortgage payments. This led to a wave of foreclosures, which is when the bank takes back a home because the owner can't make the payments. As a result, many banks and financial institutions lost a lot of money because they had invested in these mortgages and were now unable to recoup their investments. This caused a financial crisis, as the banks and financial institutions that had lost money were no longer able to lend money to other people or businesses. The money that was lost during the financial crisis did not simply disappear. Instead, it was lost by the banks and financial institutions that had invested in the subprime mortgages and were unable to recoup their investments when the housing market crashed. This loss of money had a ripple effect, as the banks and financial institutions were then unable to lend money to other people or businesses, which slowed down the economy and led to a global recession.]\\nPlease rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the helpfulness. Please first output a single line containing the value indicating the scores.\"\n",
    "num_tokens = erniebot.utils.token_helper.approx_num_tokens(test_text)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = erniebot.ChatCompletion.create(\n",
    "    model='ernie-bot',\n",
    "    messages=[{'role': 'user', 'content': test_text,\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.translate\n",
    "\n",
    "sub_text = \"故郷の桜\"\n",
    "translate_text = argostranslate.translate.translate(sub_text, \"ja\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "from_code = \"ko\"\n",
    "to_code = \"en\"\n",
    "\n",
    "# Download and install Argos Translate package\n",
    "# argostranslate.package.update_package_index()\n",
    "# available_packages = argostranslate.package.get_available_packages()\n",
    "# package_to_install = next(\n",
    "#     filter(\n",
    "#         lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
    "#     )\n",
    "# )\n",
    "# argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "# Translate\n",
    "translatedText = argostranslate.translate.translate(\"다음의 title과 abstract를 가지고 논문을 쓸거야. 개요를 introduction, material and method, result, discussion, conclusion 순으로 heading과 subheading 형태를 갖는 outline을 작성해줘 : Title: Synergistic Effect of Statins and Aspirin on Adhesion Prevention Following Abdominal Surgery: A Retrospective Cohort Study\\n\\nAbstract:\\nIntroduction:\\nPostoperative adhesion formation is a common complication after abdominal surgery that can lead to chronic pain, bowel obstruction, and infertility. Statins, commonly used to treat hyperlipidemia, have been suggested to have a role in preventing adhesions by modulating the inflammatory response, promoting collagen remodeling, and inhibiting fibroblast proliferation. Combining statins with other medications commonly used during and after abdominal surgery may lead to a synergistic effect on adhesion prevention. In this study, we aimed to investigate the potential synergistic effect of statins when used in combination with other medications on adhesion prevention in patients undergoing abdominal surgery.\\nMaterials and Methods:\\nWe conducted a retrospective cohort study including 500 patients who underwent abdominal surgery between 2018 and 2021. Patients were divided into four groups based on medication use: no medication, statin, aspirin, and statin + aspirin. The incidence of postoperative adhesions was assessed at 1-year follow-up.\\nResults:\\nAmong the 500 patients included in the study, the incidence of postoperative adhesions was highest in the no medication group (30%), followed by the aspirin group (25%), the statin group (20%), and the statin + aspirin group (10%). The statin + aspirin group had the lowest incidence of postoperative adhesions compared to the other groups.\", from_code, to_code)\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translatedText = argostranslate.translate.translate(\"다음의 title과 abstract를 가지고 논문을 쓸거야. 개요를 introduction, material and method, result, discussion, conclusion 순으로 heading과 subheading 형태를 갖는 outline을 작성해줘 : Title: Synergistic Effect of Statins and Aspirin on Adhesion Prevention Following Abdominal Surgery: A Retrospective Cohort Study\\n\\nAbstract:\\nIntroduction:\\nPostoperative adhesion formation is a common complication after abdominal surgery that can lead to chronic pain, bowel obstruction, and infertility. Statins, commonly used to treat hyperlipidemia, have been suggested to have a role in preventing adhesions by modulating the inflammatory response, promoting collagen remodeling, and inhibiting fibroblast proliferation. Combining statins with other medications commonly used during and after abdominal surgery may lead to a synergistic effect on adhesion prevention. In this study, we aimed to investigate the potential synergistic effect of statins when used in combination with other medications on adhesion prevention in patients undergoing abdominal surgery.\\nMaterials and Methods:\\nWe conducted a retrospective cohort study including 500 patients who underwent abdominal surgery between 2018 and 2021. Patients were divided into four groups based on medication use: no medication, statin, aspirin, and statin + aspirin. The incidence of postoperative adhesions was assessed at 1-year follow-up.\\nResults:\\nAmong the 500 patients included in the study, the incidence of postoperative adhesions was highest in the no medication group (30%), followed by the aspirin group (25%), the statin group (20%), and the statin + aspirin group (10%). The statin + aspirin group had the lowest incidence of postoperative adhesions compared to the other groups.\", from_code, to_code)\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v4_en_2023-11-11-17-37-38\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"model = tf.keras.Sequential([\\n layers.Embedding(num\\\\_words, embedding\\\\_dim, embeddings\\\\_initializer=tf.keras.initializers.Constant(embedding\\\\_matrix),\\n input\\\\_length=max\\\\_length, trainable=False),\\n layers.Conv1D(128, 5, activation='relu'),\\n layers.MaxPooling1D(pool\\\\_size=4),\\n layers.Flatten(),\\n layers.Dense(1, activation='sigmoid')\\n])\\n\\nwe have used this model with glove embedding to perform classification..can you explain the model and what each layer of the model does...our accuracy is 84 right now so how do we improve the accuracy by enhancing the model\"\n",
    "gen_kwargs = {\"max_length\": 1024, \"top_p\": 0.8, \"temperature\": 0.8, \"repetition_penalty\": 1.1, \"do_sample\": True}\n",
    "inp_text = tokenizer([text], return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(inp_text, **gen_kwargs)\n",
    "output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "output_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'Dataset': 'instruct', 'Gen': 'COL', 'IFT': 'False', 'Lang': 'EN', 'CFT-MR': 'False', 'CFT-P': 'False', 'CFT-SR': 'True', 'Task': 'MT', 'original_path': 'Alpaca-CoT/instruct/instruct.json'}, 'text': 'Question: \"What could a company do to get around the restrictions on the number of parts?\"\\n\\nContext: \"Nintendo was not as restrictive as Sega, which did not permit third-party publishing until Mediagenic in late summer 1988. Nintendo\\'s intention, however, was to reserve a large part of NES game revenue for itself. Nintendo required that they be the sole manufacturer of all cartridges, and that the publisher had to pay in full before the cartridges for that game be produced. Cartridges could not be returned to Nintendo, so publishers assumed all the risk. As a result, some publishers lost more money due to distress sales of remaining inventory at the end of the NES era than they ever earned in profits from sales of the games. Because Nintendo controlled the production of all cartridges, it was able to enforce strict rules on its third-party developers, which were required to sign a contract by Nintendo that would obligate these parties to develop exclusively for the system, order at least 10,000 cartridges, and only make five games per year. A 1988 shortage of DRAM and ROM chips also reportedly caused Nintendo to only permit 25% of publishers\\' requests for cartridges. This was an average figure, with some publishers receiving much higher amounts and others almost none. GameSpy noted that Nintendo\\'s \"iron-clad terms\" made the company many enemies during the 1980s. Some developers tried to circumvent the five game limit by creating additional company brands like Konami\\'s Ultra Games label; others tried circumventing the 10NES chip.\"\\n\\nAnswer:\\n  A company could try to circumvent the restrictions on the number of parts by creating additional company brands or labels, as well as trying to bypass the 10NES chip. However, these actions could potentially lead to legal issues with Nintendo. Another option may be to negotiate with Nintendo for more lenient terms or to develop games on other platforms.', 'input': '', 'output': 'A company could try to circumvent the restrictions on the number of parts by creating additional company brands or labels, as well as trying to bypass the 10NES chip. However, these actions could potentially lead to legal issues with Nintendo. Another option may be to negotiate with Nintendo for more lenient terms or to develop games on other platforms.', 'instruction': 'Question: \"What could a company do to get around the restrictions on the number of parts?\"\\n\\nContext: \"Nintendo was not as restrictive as Sega, which did not permit third-party publishing until Mediagenic in late summer 1988. Nintendo\\'s intention, however, was to reserve a large part of NES game revenue for itself. Nintendo required that they be the sole manufacturer of all cartridges, and that the publisher had to pay in full before the cartridges for that game be produced. Cartridges could not be returned to Nintendo, so publishers assumed all the risk. As a result, some publishers lost more money due to distress sales of remaining inventory at the end of the NES era than they ever earned in profits from sales of the games. Because Nintendo controlled the production of all cartridges, it was able to enforce strict rules on its third-party developers, which were required to sign a contract by Nintendo that would obligate these parties to develop exclusively for the system, order at least 10,000 cartridges, and only make five games per year. A 1988 shortage of DRAM and ROM chips also reportedly caused Nintendo to only permit 25% of publishers\\' requests for cartridges. This was an average figure, with some publishers receiving much higher amounts and others almost none. GameSpy noted that Nintendo\\'s \"iron-clad terms\" made the company many enemies during the 1980s. Some developers tried to circumvent the five game limit by creating additional company brands like Konami\\'s Ultra Games label; others tried circumventing the 10NES chip.\"\\n\\nAnswer:', '__dj__hash': '89434a82b636e969b4852fade6dee4c2', '__dj__stats__': {'alnum_ratio': 0.8121748179, 'avg_line_length': 320.3333333333, 'char_rep_ratio': 0.0538421328, 'flagged_words_ratio': 0.0, 'instruction_num_words': 251, 'lang': 'en', 'lang_score': 0.977707386, 'max_line_length': 1463, 'num_token': 379, 'num_words': 308, 'output_num_words': 59, 'perplexity': 430.4, 'text_len': 1922, 'word_rep_ratio': 0.0}, '__dj__simhash': 1.534570002e+19}\n",
      "{'__dj__stats__': {'alnum_ratio': 0.8121748179, 'avg_line_length': 320.3333333333, 'char_rep_ratio': 0.0538421328, 'flagged_words_ratio': 0.0, 'instruction_num_words': 251, 'lang': 'en', 'lang_score': 0.977707386, 'max_line_length': 1463, 'num_token': 379, 'num_words': 308, 'output_num_words': 59, 'perplexity': 430.4, 'text_len': 1922, 'word_rep_ratio': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "stats_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v9_from_v4_en_2023-11-18-15-00-17/data/en/datasets_en_stats.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v9_from_v4_en_2023-11-18-15-00-17/data/en/datasets_en.jsonl\"\n",
    "stat_reader = jsonlines.open(stats_path, 'r')\n",
    "data_reader = jsonlines.open(data_path, 'r')\n",
    "\n",
    "for stat, data in zip(stat_reader, data_reader):\n",
    "    print(data)\n",
    "    print(stat)\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dj_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
