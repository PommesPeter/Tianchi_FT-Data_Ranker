{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "# EN\n",
    "# len: 4459532\n",
    "# key per line: dict_keys(['meta', 'text', 'input', 'output', 'instruction'])\n",
    "# 'meta' key: dict_keys(['Dataset', 'Gen', 'IFT', 'Lang', 'CFT-MR', 'CFT-P', 'CFT-SR', 'Task', 'original_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'ShareGPT', 'belle_cn', 'instinwild', 'Chinese-medical', 'alpacaGPT4', 'COIG', 'HC3', 'MOSS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "data_stat_path = \"/home/chenxiaojia/data/competition/competition_kit/data-juicer/outputs/v6_text/en_processed_stats.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/training_dataset.jsonl\"\n",
    "output = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/flitered_training_dataset.jsonl\"\n",
    "\n",
    "cnt = 0\n",
    "new_data_writer = jsonlines.open(output, 'w')\n",
    "\n",
    "with jsonlines.open(data_stat_path) as file1, jsonlines.open(data_path) as file2:\n",
    "    for data1, data2 in zip(file1, file2):\n",
    "        # process data from both files here\n",
    "        data1 = data1[\"__dj__stats__\"]\n",
    "        if data1[\"flagged_words_ratio\"] <= 0.017:\n",
    "            # print(data1[\"flagged_words_ratio\"])\n",
    "            # print(data2)\n",
    "            # print(\"\")\n",
    "            cnt += 1\n",
    "            new_data_writer.write(data2)\n",
    "\n",
    "new_data_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/raw_data/raw_data_en.jsonl\"\n",
    "\n",
    "dataset_type = {}\n",
    "task_type = {}\n",
    "reader = jsonlines.open(data_path, 'r')\n",
    "\n",
    "new_r = jsonlines.open(\"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/debug_dj/debug_1k\", 'w')\n",
    "\n",
    "cnt = 0\n",
    "for line in reader:\n",
    "    if cnt <= 1000:\n",
    "        new_r.write(line)\n",
    "        cnt += 1\n",
    "new_r.close()\n",
    "# for line in reader:\n",
    "#     ds_type = line['meta']['Dataset']\n",
    "#     tsk_type = line['meta']['Task']\n",
    "#     if dataset_type.get(ds_type, -1) == -1:\n",
    "#         dataset_type[ds_type] = 0\n",
    "#         task_type[tsk_type] = 0\n",
    "#     dataset_type[ds_type] += 1\n",
    "#     task_type[tsk_type] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in dataset_type.items():\n",
    "#     print(k, len(v))\n",
    "\n",
    "print(task_type)\n",
    "print(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('temp_data/sharegpt.json', 'w') as f:\n",
    "    json.dump(dataset_type['ShareGPT'], f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(open('temp_data/sharegpt.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM for evaluation data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ernie\n",
    "import erniebot\n",
    "\n",
    "erniebot.api_type = 'aistudio'\n",
    "erniebot.access_token = 'a335b39380dbc8aaaec76f78725d1dc867cf2d27'\n",
    "\n",
    "models = erniebot.Model.list()\n",
    "print(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following.\\nInstruction: [where did all the money that was lost in the sub prime mortgage crisis and subsequent global recession actually go ? Money does n't simply disappear so if people lost money then it must have gone somewhere . Where ? Explain like I'm five.]\\nInput: []\\nResponse: [The subprime mortgage crisis was a financial crisis that occurred in the late 2000s as a result of the collapse of the housing market. Many people had taken out mortgages to buy homes, but when the housing market crashed, the value of their homes decreased and they were no longer able to make their mortgage payments. This led to a wave of foreclosures, which is when the bank takes back a home because the owner can't make the payments. As a result, many banks and financial institutions lost a lot of money because they had invested in these mortgages and were now unable to recoup their investments. This caused a financial crisis, as the banks and financial institutions that had lost money were no longer able to lend money to other people or businesses. The money that was lost during the financial crisis did not simply disappear. Instead, it was lost by the banks and financial institutions that had invested in the subprime mortgages and were unable to recoup their investments when the housing market crashed. This loss of money had a ripple effect, as the banks and financial institutions were then unable to lend money to other people or businesses, which slowed down the economy and led to a global recession.]\\nPlease rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the helpfulness. Please first output a single line containing the value indicating the scores.\"\n",
    "num_tokens = erniebot.utils.token_helper.approx_num_tokens(test_text)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = erniebot.ChatCompletion.create(\n",
    "    model='ernie-bot',\n",
    "    messages=[{'role': 'user', 'content': test_text,\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.translate\n",
    "\n",
    "sub_text = \"故郷の桜\"\n",
    "translate_text = argostranslate.translate.translate(sub_text, \"ja\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argostranslate.package\n",
    "import argostranslate.translate\n",
    "\n",
    "from_code = \"ko\"\n",
    "to_code = \"en\"\n",
    "\n",
    "# Download and install Argos Translate package\n",
    "# argostranslate.package.update_package_index()\n",
    "# available_packages = argostranslate.package.get_available_packages()\n",
    "# package_to_install = next(\n",
    "#     filter(\n",
    "#         lambda x: x.from_code == from_code and x.to_code == to_code, available_packages\n",
    "#     )\n",
    "# )\n",
    "# argostranslate.package.install_from_path(package_to_install.download())\n",
    "\n",
    "# Translate\n",
    "translatedText = argostranslate.translate.translate(\"다음의 title과 abstract를 가지고 논문을 쓸거야. 개요를 introduction, material and method, result, discussion, conclusion 순으로 heading과 subheading 형태를 갖는 outline을 작성해줘 : Title: Synergistic Effect of Statins and Aspirin on Adhesion Prevention Following Abdominal Surgery: A Retrospective Cohort Study\\n\\nAbstract:\\nIntroduction:\\nPostoperative adhesion formation is a common complication after abdominal surgery that can lead to chronic pain, bowel obstruction, and infertility. Statins, commonly used to treat hyperlipidemia, have been suggested to have a role in preventing adhesions by modulating the inflammatory response, promoting collagen remodeling, and inhibiting fibroblast proliferation. Combining statins with other medications commonly used during and after abdominal surgery may lead to a synergistic effect on adhesion prevention. In this study, we aimed to investigate the potential synergistic effect of statins when used in combination with other medications on adhesion prevention in patients undergoing abdominal surgery.\\nMaterials and Methods:\\nWe conducted a retrospective cohort study including 500 patients who underwent abdominal surgery between 2018 and 2021. Patients were divided into four groups based on medication use: no medication, statin, aspirin, and statin + aspirin. The incidence of postoperative adhesions was assessed at 1-year follow-up.\\nResults:\\nAmong the 500 patients included in the study, the incidence of postoperative adhesions was highest in the no medication group (30%), followed by the aspirin group (25%), the statin group (20%), and the statin + aspirin group (10%). The statin + aspirin group had the lowest incidence of postoperative adhesions compared to the other groups.\", from_code, to_code)\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translatedText = argostranslate.translate.translate(\"다음의 title과 abstract를 가지고 논문을 쓸거야. 개요를 introduction, material and method, result, discussion, conclusion 순으로 heading과 subheading 형태를 갖는 outline을 작성해줘 : Title: Synergistic Effect of Statins and Aspirin on Adhesion Prevention Following Abdominal Surgery: A Retrospective Cohort Study\\n\\nAbstract:\\nIntroduction:\\nPostoperative adhesion formation is a common complication after abdominal surgery that can lead to chronic pain, bowel obstruction, and infertility. Statins, commonly used to treat hyperlipidemia, have been suggested to have a role in preventing adhesions by modulating the inflammatory response, promoting collagen remodeling, and inhibiting fibroblast proliferation. Combining statins with other medications commonly used during and after abdominal surgery may lead to a synergistic effect on adhesion prevention. In this study, we aimed to investigate the potential synergistic effect of statins when used in combination with other medications on adhesion prevention in patients undergoing abdominal surgery.\\nMaterials and Methods:\\nWe conducted a retrospective cohort study including 500 patients who underwent abdominal surgery between 2018 and 2021. Patients were divided into four groups based on medication use: no medication, statin, aspirin, and statin + aspirin. The incidence of postoperative adhesions was assessed at 1-year follow-up.\\nResults:\\nAmong the 500 patients included in the study, the incidence of postoperative adhesions was highest in the no medication group (30%), followed by the aspirin group (25%), the statin group (20%), and the statin + aspirin group (10%). The statin + aspirin group had the lowest incidence of postoperative adhesions compared to the other groups.\", from_code, to_code)\n",
    "print(translatedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v4_en_2023-11-11-17-37-38\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"model = tf.keras.Sequential([\\n layers.Embedding(num\\\\_words, embedding\\\\_dim, embeddings\\\\_initializer=tf.keras.initializers.Constant(embedding\\\\_matrix),\\n input\\\\_length=max\\\\_length, trainable=False),\\n layers.Conv1D(128, 5, activation='relu'),\\n layers.MaxPooling1D(pool\\\\_size=4),\\n layers.Flatten(),\\n layers.Dense(1, activation='sigmoid')\\n])\\n\\nwe have used this model with glove embedding to perform classification..can you explain the model and what each layer of the model does...our accuracy is 84 right now so how do we improve the accuracy by enhancing the model\"\n",
    "gen_kwargs = {\"max_length\": 1024, \"top_p\": 0.8, \"temperature\": 0.8, \"repetition_penalty\": 1.1, \"do_sample\": True}\n",
    "inp_text = tokenizer([text], return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(inp_text, **gen_kwargs)\n",
    "output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = tf.keras.Sequential([\\n layers.Embedding(num\\\\_words, embedding\\\\_dim, embeddings\\\\_initializer=tf.keras.initializers.Constant(embedding\\\\_matrix),\\n input\\\\_length=max\\\\_length, trainable=False),\\n layers.Conv1D(128, 5, activation='relu'),\\n layers.MaxPooling1D(pool\\\\_size=4),\\n layers.Flatten(),\\n layers.Dense(1, activation='sigmoid')\\n])\\n\\nwe have used this model with glove embedding to perform classification..can you explain the model and what each layer of the model does...our accuracy is 84 right now so how do we improve the accuracy by enhancing the model?\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('transformer',\n",
       "               FalconModel(\n",
       "                 (word_embeddings): Embedding(50258, 2048)\n",
       "                 (h): ModuleList(\n",
       "                   (0-23): 24 x FalconDecoderLayer(\n",
       "                     (self_attention): FalconAttention(\n",
       "                       (query_key_value): FalconLinear(in_features=2048, out_features=6144, bias=True)\n",
       "                       (dense): FalconLinear(in_features=2048, out_features=2048, bias=True)\n",
       "                       (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (mlp): FalconMLP(\n",
       "                       (dense_h_to_4h): FalconLinear(in_features=2048, out_features=8192, bias=True)\n",
       "                       (act): GELU(approximate='none')\n",
       "                       (dense_4h_to_h): FalconLinear(in_features=8192, out_features=2048, bias=True)\n",
       "                     )\n",
       "                     (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                     (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                   )\n",
       "                 )\n",
       "                 (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=2048, out_features=50258, bias=False))]),\n",
       " 'config': FalconConfig {\n",
       "   \"_name_or_path\": \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v4_en_2023-11-11-17-37-38\",\n",
       "   \"alibi\": true,\n",
       "   \"apply_residual_connection_post_layernorm\": false,\n",
       "   \"architectures\": [\n",
       "     \"FalconForCausalLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"auto_map\": {\n",
       "     \"AutoConfig\": \"configuration_falcon.FalconConfig\",\n",
       "     \"AutoModel\": \"modeling_falcon.FalconModel\",\n",
       "     \"AutoModelForCausalLM\": \"modeling_falcon.FalconForCausalLM\",\n",
       "     \"AutoModelForQuestionAnswering\": \"modeling_falcon.FalconForQuestionAnswering\",\n",
       "     \"AutoModelForSequenceClassification\": \"modeling_falcon.FalconForSequenceClassification\",\n",
       "     \"AutoModelForTokenClassification\": \"modeling_falcon.FalconForTokenClassification\"\n",
       "   },\n",
       "   \"bias\": true,\n",
       "   \"bos_token_id\": 50256,\n",
       "   \"eos_token_id\": 50256,\n",
       "   \"hidden_dropout\": 0.0,\n",
       "   \"hidden_size\": 2048,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"max_position_embeddings\": 2048,\n",
       "   \"model_type\": \"falcon\",\n",
       "   \"multi_query\": false,\n",
       "   \"new_decoder_architecture\": false,\n",
       "   \"num_attention_heads\": 32,\n",
       "   \"num_hidden_layers\": 24,\n",
       "   \"num_kv_heads\": 32,\n",
       "   \"pad_token_id\": 50257,\n",
       "   \"parallel_attn\": false,\n",
       "   \"rope_scaling\": null,\n",
       "   \"rope_theta\": 10000.0,\n",
       "   \"torch_dtype\": \"bfloat16\",\n",
       "   \"transformers_version\": \"4.34.1\",\n",
       "   \"unk_token_id\": 50256,\n",
       "   \"use_cache\": false,\n",
       "   \"vocab_size\": 50258\n",
       " },\n",
       " 'name_or_path': '/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v4_en_2023-11-11-17-37-38',\n",
       " 'warnings_issued': {},\n",
       " 'generation_config': GenerationConfig {\n",
       "   \"bos_token_id\": 50256,\n",
       "   \"eos_token_id\": 50256,\n",
       "   \"pad_token_id\": 50257,\n",
       "   \"use_cache\": false\n",
       " },\n",
       " '_is_hf_initialized': True,\n",
       " 'is_loaded_in_4bit': False,\n",
       " 'is_loaded_in_8bit': False}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dj_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
