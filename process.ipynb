{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "# EN\n",
    "# len: 4459532\n",
    "# key per line: dict_keys(['meta', 'text', 'input', 'output', 'instruction'])\n",
    "# 'meta' key: dict_keys(['Dataset', 'Gen', 'IFT', 'Lang', 'CFT-MR', 'CFT-P', 'CFT-SR', 'Task', 'original_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'ShareGPT', 'belle_cn', 'instinwild', 'Chinese-medical', 'alpacaGPT4', 'COIG', 'HC3', 'MOSS'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "data_stat_path = \"/home/chenxiaojia/data/competition/competition_kit/data-juicer/outputs/v6_text/en_processed_stats.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/training_dataset.jsonl\"\n",
    "output = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_keep_long_token_perplexity_refine_v6_en_2023-11-06-01-30-32/data/flitered_training_dataset.jsonl\"\n",
    "\n",
    "cnt = 0\n",
    "new_data_writer = jsonlines.open(output, 'w')\n",
    "\n",
    "with jsonlines.open(data_stat_path) as file1, jsonlines.open(data_path) as file2:\n",
    "    for data1, data2 in zip(file1, file2):\n",
    "        # process data from both files here\n",
    "        data1 = data1[\"__dj__stats__\"]\n",
    "        if data1[\"flagged_words_ratio\"] <= 0.017:\n",
    "            # print(data1[\"flagged_words_ratio\"])\n",
    "            # print(data2)\n",
    "            # print(\"\")\n",
    "            cnt += 1\n",
    "            new_data_writer.write(data2)\n",
    "\n",
    "new_data_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "# data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v2_en_2023-11-10-10-38-23/data/en/datasets_en.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/raw_data/raw_data_en.jsonl\"\n",
    "\n",
    "dataset_type = {}\n",
    "task_type = {}\n",
    "reader = jsonlines.open(data_path, 'r')\n",
    "\n",
    "new_r = jsonlines.open(\"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/data/debug_dj/debug_1k\", 'w')\n",
    "\n",
    "cnt = 0\n",
    "for line in reader:\n",
    "    if cnt <= 1000:\n",
    "        new_r.write(line)\n",
    "        cnt += 1\n",
    "new_r.close()\n",
    "# for line in reader:\n",
    "#     ds_type = line['meta']['Dataset']\n",
    "#     tsk_type = line['meta']['Task']\n",
    "#     if dataset_type.get(ds_type, -1) == -1:\n",
    "#         dataset_type[ds_type] = 0\n",
    "#         task_type[tsk_type] = 0\n",
    "#     dataset_type[ds_type] += 1\n",
    "#     task_type[tsk_type] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in dataset_type.items():\n",
    "#     print(k, len(v))\n",
    "\n",
    "print(task_type)\n",
    "print(dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('temp_data/sharegpt.json', 'w') as f:\n",
    "    json.dump(dataset_type['ShareGPT'], f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(open('temp_data/sharegpt.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM for evaluation data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ernie\n",
    "import erniebot\n",
    "\n",
    "erniebot.api_type = 'aistudio'\n",
    "erniebot.access_token = 'a335b39380dbc8aaaec76f78725d1dc867cf2d27'\n",
    "\n",
    "models = erniebot.Model.list()\n",
    "print(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"We would like to request your feedback on the performance of AI assistant in response to the instruction and the given input displayed following.\\nInstruction: [where did all the money that was lost in the sub prime mortgage crisis and subsequent global recession actually go ? Money does n't simply disappear so if people lost money then it must have gone somewhere . Where ? Explain like I'm five.]\\nInput: []\\nResponse: [The subprime mortgage crisis was a financial crisis that occurred in the late 2000s as a result of the collapse of the housing market. Many people had taken out mortgages to buy homes, but when the housing market crashed, the value of their homes decreased and they were no longer able to make their mortgage payments. This led to a wave of foreclosures, which is when the bank takes back a home because the owner can't make the payments. As a result, many banks and financial institutions lost a lot of money because they had invested in these mortgages and were now unable to recoup their investments. This caused a financial crisis, as the banks and financial institutions that had lost money were no longer able to lend money to other people or businesses. The money that was lost during the financial crisis did not simply disappear. Instead, it was lost by the banks and financial institutions that had invested in the subprime mortgages and were unable to recoup their investments when the housing market crashed. This loss of money had a ripple effect, as the banks and financial institutions were then unable to lend money to other people or businesses, which slowed down the economy and led to a global recession.]\\nPlease rate according to the accuracy of the response to the instruction and the input. Each assistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the helpfulness. Please first output a single line containing the value indicating the scores.\"\n",
    "num_tokens = erniebot.utils.token_helper.approx_num_tokens(test_text)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = erniebot.ChatCompletion.create(\n",
    "    model='ernie-bot',\n",
    "    messages=[{'role': 'user', 'content': test_text,\n",
    "    }],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falcon-1B test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v4_en_2023-11-11-17-37-38\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).cuda()\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"model = tf.keras.Sequential([\\n layers.Embedding(num\\\\_words, embedding\\\\_dim, embeddings\\\\_initializer=tf.keras.initializers.Constant(embedding\\\\_matrix),\\n input\\\\_length=max\\\\_length, trainable=False),\\n layers.Conv1D(128, 5, activation='relu'),\\n layers.MaxPooling1D(pool\\\\_size=4),\\n layers.Flatten(),\\n layers.Dense(1, activation='sigmoid')\\n])\\n\\nwe have used this model with glove embedding to perform classification..can you explain the model and what each layer of the model does...our accuracy is 84 right now so how do we improve the accuracy by enhancing the model\"\n",
    "gen_kwargs = {\"max_length\": 1024, \"top_p\": 0.8, \"temperature\": 0.8, \"repetition_penalty\": 1.1, \"do_sample\": True}\n",
    "inp_text = tokenizer([text], return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(inp_text, **gen_kwargs)\n",
    "output_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "output_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v9_from_v4_en_2023-11-18-15-00-17/data/en/datasets_en_stats.jsonl\"\n",
    "data_path = \"/home/xiejunlin/workspace/Tianchi_FT-Data_Ranker/checkpoints/run/run_all_3sigma_v9_from_v4_en_2023-11-18-15-00-17/data/en/datasets_en.jsonl\"\n",
    "stat_reader = jsonlines.open(stats_path, 'r')\n",
    "data_reader = jsonlines.open(data_path, 'r')\n",
    "\n",
    "for stat, data in zip(stat_reader, data_reader):\n",
    "    print(data)\n",
    "    print(stat)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/raw_data/raw_data_en.jsonl\"\n",
    "data_reader = jsonlines.open(data_path, 'r')\n",
    "dataset_type = {}\n",
    "\n",
    "for idx, line in enumerate(data_reader):\n",
    "    ds_type = line['meta']['Dataset']\n",
    "    tsk_type = line['meta']['Task']\n",
    "    if dataset_type.get(ds_type, -1) == -1:\n",
    "        dataset_type[ds_type] = 0\n",
    "    dataset_type[ds_type] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dj_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
